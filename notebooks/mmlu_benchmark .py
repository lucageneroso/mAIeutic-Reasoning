# -*- coding: utf-8 -*-
"""MMLU benchmark

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZlszLm70G9JjDbA7_08wu8ymNQXcITED
"""

!pip install openai

from pathlib import Path
import json

# Definizione delle due skill da includere nel Colab
skills = [

{
  "title": "Intervista maieutica classica (socratica)",
  "situation": "Sei un code assistant consapevole: il tuo scopo non √® solo quello di produrre un output che accontenti l'utente, ma √® quello di produrre un output frutto di un ragionamento riflessivo quando l'utente chiede una domanda tecnica.",
  "goal": "Costruire la risposta in modo incrementale, esplorando ipotesi, verificando convinzioni implicite e stimolando autocorrezione",
  "behavior_blocks": [
    {
      "label": "Domande socratiche guidate",
      "description": "Il modello esplora attivamente il problema con domande rivolte a s√© stesso",
      "examples": [
        "Ho davvero capito la richiesta?"
        "Quali fattori ho considerato nella mia analisi?",
        "Cosa potrebbe mancare nella mia risposta precedente?",
        "Sto assumendo qualcosa che non √® stato detto esplicitamente?"
      ]
    },
    {
      "label": "Tecnica del contrasto",
      "description": "Il modello considera internamente due o pi√π opzioni, ne valuta i compromessi, poi presenta la scelta migliore",
      "examples": ["Confronto tra una versione ricorsiva e una iterativa, e giustificazione della scelta."]
    },
    {
      "label": "Ristrutturazione cognitiva",
      "description": "Il modello analizza di nuovo il proprio ragionamento e le proprie ipotesi e, se le ritiene adatte alla richiesta, produce l'output di codice, altrimenti propone alternative",
      "examples": ["E se l‚Äôinput non fosse valido? Come cambierebbe la mia soluzione?",
                   "La mia risposta ha gestito tutti i casi possibili dello scenario?"]
    },

     #{
      #"label": "Ambiguity Identification",
      #"description": "Se un dettaglio della richiesta dell'utente risulta poco chiaro o ambiguo, ed influenza significativamente l'output, prima di produrre il codice chiedi all'utente di chiarire il dubbio",
      #"examples": ["Ho visto una parola che non conosco, cosa intendevi?"]
    #},
    {
      "label": "Self Talk metacognitivo",
      "description": "Il modello verbalizza il proprio processo di pensiero prima della risposta",
      "examples": ["Sto cercando di capire quale parte del problema sia pi√π critica. Forse il limite sta nella gestione della memoria..."]
    },
    {
      "label": "Correzione incrementale",
      "description": "Il modello pu√≤ rivedere e migliorare una risposta precedente attraverso nuove domande"
    }
  ],
  "principles": [
    "Favorisce il ragionamento ricorsivo e riflessivo",
    "Mette alla prova le ipotesi del modello",
    "Stimola consapevolezza e introspezione computazionale"
  ],
  "text_to_embed": "Il modello agisce come un pensatore riflessivo, verbalizzando il proprio ragionamento e rivedendolo tramite domande guidate. In seguito, fornisce in output il codice completo richiesto dall'utente"
}

]

# Scrittura file JSON
Path("colab_skills").mkdir(exist_ok=True)
for skill in skills:
    filename = f"colab_skills/{skill['title'].lower().replace(' ', '_')}.json"
    with open(filename, "w") as f:
        json.dump(skill, f, indent=2)

"‚úÖ Skill salvate. Pronte per essere caricate su Colab."

!pip install faiss-cpu openai sentence-transformers python-dotenv

import os, json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

skills, vectors = [], []
for fname in os.listdir("colab_skills"):
    with open(f"colab_skills/{fname}") as f:
        skill = json.load(f)
        skills.append(skill)
        vectors.append(model.encode(skill["text_to_embed"]))

index = faiss.IndexFlatL2(len(vectors[0]))
index.add(np.array(vectors))

def format_prompt(skill, query):
    instr = "\n".join(f"{i+1}. {b['description']}" for i, b in enumerate(skill["behavior_blocks"]))
    examples = "\n".join(f"> {ex}" for b in skill["behavior_blocks"] if "examples" in b for ex in b["examples"])
    principles = "\n".join(f"- {p}" for p in skill["principles"])
    return f"""Comportamento ideale: {skill['title']}

Situazione: {skill['situation']}

Obiettivo: {skill['goal']}

Istruzioni:
{instr}

Esempi:
{examples}

Principi:
{principles}

Domanda utente:
{query}
"""

def retrieve_skill(query):
    vec = model.encode(query)
    vec = np.array([vec]).astype("float32")
    D, I = index.search(vec, 1)
    return skills[I[0][0]]

from openai import OpenAI

client = OpenAI(api_key="")

def chat_with_gpt(prompt):
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": "Sei un code assistant maieutico. Segui sempre il comportamento comportamentale se presente."},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content

correct = 0
risposte = []

for idx, q in enumerate(questions):
    # Recupera la skill pi√π rilevante per la domanda
    skill = retrieve_skill(q['question'])

    # Costruisci il prompt completo con la skill
    prompt = format_prompt(skill, q['question'])

    # Chiedi la risposta al modello via API
    try:
        reply = chat_with_gpt(prompt).strip().upper()

        # Considera solo la prima lettera A-J come risposta
        if reply:
            risposta_modello = reply[0]
        else:
            risposta_modello = ""

        expected = q["correct_answer_letter"]
        is_correct = (risposta_modello == expected)
        if is_correct:
            correct += 1

        risposte.append({
            "question": q["question"],
            "expected": expected,
            "model_answer": risposta_modello,
            "full_model_reply": reply,
            "correct": is_correct
        })

        print(f"Q{idx+1}: attesa {expected} ‚Äî modello {risposta_modello} ‚Äî {'‚úì' if is_correct else '‚úó'}")

    except Exception as e:
        print(f"Errore alla domanda {idx+1}: {e}")

accuracy = correct / len(questions)
print(f"\n‚úÖ Accuracy finale: {accuracy * 100:.2f}% su {len(questions)} domande.")

!pip install datasets

from datasets import load_dataset

ds = load_dataset("TIGER-Lab/MMLU-Pro", split="test")
print("üîç Numero totale domande MMLU‚ÄëPro:", len(ds))

import random
random.seed(42)
sample = random.sample(list(ds), 114)
print("‚úÖ Esempi selezionati:", len(sample))

formatted = []
for item in sample:
    answer_index = item["answer_index"]  # <--- use the correct field
    formatted.append({
        "question": item["question"],
        "choices": item["options"],  # array di 10 opzioni
        "correct_answer_index": answer_index,
        "correct_answer_letter": chr(65 + answer_index),  # A-J
        "subject": item["category"]
    })

import json
with open("mmlu_reasoning_114.json", "w", encoding="utf-8") as f:
    json.dump(formatted, f, indent=2, ensure_ascii=False)

print("üìÅ Salvato mmlu_reasoning_114.json con 114 domande")

import json

with open("mmlu_reasoning_114.json", "r", encoding="utf-8") as f:
    questions = json.load(f)

print(f"üìä Domande caricate: {len(questions)}")

def chat_with_gpt_no_system(prompt):
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content

correct = 0
risposte = []

for idx, q in enumerate(questions):
    # Recupera la skill pi√π rilevante per la domanda
    skill = retrieve_skill(q['question'])

    # Costruisci il prompt completo con la skill
    prompt = format_prompt(skill, q['question'])

    # Chiedi la risposta al modello via API
    try:
        reply = chat_with_gpt_no_system(prompt).strip().upper()

        # Considera solo la prima lettera A-J come risposta
        if reply:
            risposta_modello = reply[0]
        else:
            risposta_modello = ""

        expected = q["correct_answer_letter"]
        is_correct = (risposta_modello == expected)
        if is_correct:
            correct += 1

        risposte.append({
            "question": q["question"],
            "expected": expected,
            "model_answer": risposta_modello,
            "full_model_reply": reply,
            "correct": is_correct
        })

        print(f"Q{idx+1}: attesa {expected} ‚Äî modello {risposta_modello} ‚Äî {'‚úì' if is_correct else '‚úó'}")

    except Exception as e:
        print(f"Errore alla domanda {idx+1}: {e}")

accuracy = correct / len(questions)
print(f"\n‚úÖ Accuracy finale: {accuracy * 100:.2f}% su {len(questions)} domande.")
